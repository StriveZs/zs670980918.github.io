---
titile: word embedding

categories:
  - Knowledge
  - Nerual Network

tags:
  - word embedding
---

# Word Embedding
## Introduction
### 引入One-Hot
自然语言处理主要研究语言信息，语言（词、句子等）属于人类认知过程中产生的高层认知抽象实体，而语言和图像属于较低层的元时输入信号。语音、图像和数据表达不需要特殊的编码，并且有天生的顺序性和关联性，近似的数字会被认为是近似的特征集合。正如图像是由像素的组成，语言是由词或字组成，可以把语言转换为词或字表示的结合。  
然后，不同于像素的大小天生具有色彩信息，词的数值大小很难表征词的含义。最后，人们为了方便，**采用One-Hot编码格式**，以一个只有10个不同词的语料库为例，我们可以用一个10位的向量来表示每个词，该向量在词下标位置的值为1，而其他全部为0.

```
第一个词：[1,0,0,0,0,0,0,0,0,0]
第二个词: [0,1,0,0,0,0,0,0,0,0]
·····
第十个词：[0,0,0,0,0,0,0,0,0,1]
```

这种词的表示方法十分简单，也很容易实现，充分解决了分类器难以处理属性数据的问题。但是它的**缺点**也很明显：冗余太多、无法体现词与词之间的关系。可以看到，这10个词的表示，彼此之间都是相互正交的。同时，随着词数的，One-Hot向量的维度也会急剧增长，如果有3000个不同的词，那么每个One-Hot词向量都是3k维，而且只有一个位置为1，其余位置都是0。（**这里感觉为了节省的可以采用（300，3000）来表示长度为3k的向量第300个位置上为1，其余都为0**）。虽然One-Hot编码格式在传统任务上表现出色，但是由于词的维度太高了，应用在深度学习上时，常常出现维度灾难，++所以在深度学习中一般采用词向量的表示形式++。

### 引入词向量
词向量(Word Vector)，也称为词嵌入(Word Embedding),并没有严格统一的定义。从概念上讲，**它是指把一个维数为所有词的数量的高维空间(几万几十万个字)嵌入一个维数低得多的连续向量空间(128或256维)中，每个单词或词组被映射为实数域上的向量**。  
词向量有专门的训练方法，这里不会细讲，感兴趣的可以学习斯坦福的CS224系列课程。这里只需要了解**词向量最重要的特征是相似词的词向量距离相近。每个词的词向量维度都是固定的，每一维都是连续的数。**

举个例子，如果我们用二维的词向量表示十个词：足球、比赛、教练、队伍、裤子、长裤、上衣和编制、折叠、拉，那么可视化出来的结果如下所示:

![figure.1](https://gitee.com/zyp521/upload_image/raw/master/YorrIg.png)

可以看出，**同类词彼此聚集，相互之间的距离较近**。

由此可见，用词向量表示词，不仅所有维度会变少（十维变为二维），其中也会包含更合理的语义信息。除了相邻词距离更近之外，词向量还有不少有趣的特征，如下图所示：

![figure.2](https://gitee.com/zyp521/upload_image/raw/master/AlftfF.png)

虚线的的两端分别是男性词和女性词，例如：叔叔和阿姨、兄弟和姐妹、先生和女士。可以看出，虚线的方向和长度都差不多，因此可以认为：国王-女王≈男人-女人，即国王可以看成男性君主，女王可以看出女性君主，国王减去男性，只剩下君主的特征，女王减去女性，也只剩下君主的特征，所以这二者近似。

## 具体介绍
英文一般是用一个向量表示一个单词，也有使用一个向量表示一个字母的情况。中文同样也有一个词或者一个字的词向量表示，与英文采用空格来区分词不同，中文的词与词之间没有间隔，因此**如果采用基于词的词向量表示，需要先进行中文分词*。

这里只对词向量做一个概要性的介绍，让我们有一个直观地认知。我们只需要掌握词向量技术用向量表证词，相似词之间的向量距离近，

### PyTorch使用
在PyTorch中，针对词向量有一个专门的层**nn.Embedding**，++用来实现词与词向量的映射++。nn.Embedding具有一个权重，形状是(num_words,embedding_dim)，例如：对上述句子中的10个词，每个词用2维向量表征，对应的权重就是一个10×2的矩阵。**Embedding的输入形状是N×W，N是batch size， W是序列长度，输出的形状是N×W×embedding_dim.输入必须是LongTensor，FloatTensor须通过tensor.long（）方法转成LongTensor**。

```
# coding:utf8
import torch as t
from torch import nn
embedding = t.nn.Embedding(10,2) # 十个词，每个词用二维词向量表示
input = t.arrange(0,6).view(3,2).long() # 三个句子，每个句子有两个词 N为3 W为2
input = t.autograd.Variable(input)
output = embedding(input)
print(output.size())
print(embedding.weight.size())
```

输出:
```
(3L,2L,2L)
(10L,2L)
```

需要注意的是，Embedding的权重也是可以训练的，既可以采用随机初始化，也可以采用预训练好的词向量初始化。

## 简单总结
可以使用torch.nn.Embedding(num_words,embedding_dim)来生成一个大小为num_words×embedding_dim词向量，  
其中共有num_words个单词，每个单词用一个二维词向量表示 类似横纵坐标 

输入到embedding中的tensor必须是LongTensor，如果是FloatTensor必须用long（）转换
Embedding的输入为N×W N是batch_size W是每个batch的单词数 如输入三个句子 每个句子包含两个单词  
Embedding的输出为N×W×embedding 即为输出还是三个句子 每个句子仍然是两个单词表示 但是每个单词则用一个二维向量表示